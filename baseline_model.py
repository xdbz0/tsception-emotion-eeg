# -*- coding: utf-8 -*-
"""baseline_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cu0tXgcF_dvhae-P6hcrvZxgdqWODE4l

mount Drive and import "data_loader"
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import sys, os
project_dir = '/content/drive/MyDrive/eeeg_emotion_detect_project'
sys.path.append(project_dir)

# %run /content/drive/MyDrive/eeeg_emotion_detect_project/data_loader.ipynb

# !jupyter nbconvert --to python data_loader.ipynb
# import data_loader as dl
CSV_PATH = os.path.join(project_dir, 'emotions.csv')

"""prepare data"""

import numpy as np
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import TensorDataset, DataLoader

# 1) read + normalization
X, y, _ = load_eeg_feature_csv(CSV_PATH)      # functions from data_loader
Xn, stats = normalize_features(X)

# 2) if USE_TSCEPTION = True, using TSception. if it is False, using FC.
USE_TSCEPTION = True
if USE_TSCEPTION:
  Xn = reshape_for_tsception_feature(Xn)      # (N,1,1,feat)

# 3) split the dataset
X_tr, X_val, y_tr, y_val = train_test_split(
  Xn, y, test_size=0.2, stratify=y, random_state=42
)

# 4) Packaged as TensorDataset
def to_tensor(arr, dtype=torch.float32):
  return torch.from_numpy(arr).to(dtype)

ds_tr  = TensorDataset(to_tensor(X_tr), torch.from_numpy(y_tr))
ds_val = TensorDataset(to_tensor(X_val), torch.from_numpy(y_val))
dl_tr  = DataLoader(ds_tr,  batch_size=64, shuffle=True)
dl_val = DataLoader(ds_val, batch_size=64)
input_dim = Xn.shape[-1]         # feat dimension，The length of the feature vector for each piece of data
num_classes = int(np.max(y) + 1)    # 3，（Negative / Neutral / Positive）

"""define the Model"""

import torch.nn as nn

if USE_TSCEPTION:
  # ----- simplify TSception（in_channels=1, only temporal kernels）-----
  class TinyTSception(nn.Module):
    def __init__(self, feat_dim, classes):
      super().__init__()
      self.conv = nn.Sequential(
          nn.Conv2d(1, 16, kernel_size=(1,3), padding=(0,1)),
          nn.ReLU(),
          nn.Conv2d(16,32, kernel_size=(1,15), padding=(0,7)),
          nn.ReLU(),
          nn.AdaptiveAvgPool2d((1,1))
      )
      self.fc = nn.Linear(32, classes)
    def forward(self, x):               # x:(B,1,1,feat)
      x = self.conv(x).view(x.size(0), -1)
      return self.fc(x)
  model = TinyTSception(input_dim, num_classes)
else:
  # ----- MLP Baseline -----
  model = nn.Sequential(
      nn.Linear(input_dim, 128),
      nn.ReLU(),
      nn.Dropout(0.2),
      nn.Linear(128, num_classes)
  )

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

"""Training and Testing"""

import torch.optim as optim
from sklearn.metrics import f1_score, accuracy_score
from tqdm import tqdm

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)
best_f1 = 0.0
EPOCHS  = 20

for epoch in range(1, EPOCHS + 1):
  # ---- Train ----
  model.train()
  for xb, yb in tqdm(dl_tr, desc=f"Epoch {epoch:02d} [train]"):
    xb, yb = xb.to(device), yb.to(device)
    optimizer.zero_grad()
    loss = criterion(model(xb), yb)
    loss.backward()
    optimizer.step()

  # ---- Validate ----
  model.eval()
  preds, gold = [], []
  with torch.no_grad():
    for xb, yb in dl_val:
      xb, yb = xb.to(device), yb.to(device)
      logits = model(xb)
      preds.extend(logits.argmax(1).cpu().numpy())
      gold.extend(yb.cpu().numpy())

  val_f1  = f1_score(gold, preds, average='macro')
  val_acc = accuracy_score(gold, preds)
  print(f"Epoch {epoch:02d}  Accuracy_value={val_acc:.3f}  f1_value={val_f1:.3f}")

  if val_f1 > best_f1:
    best_f1 = val_f1
    torch.save(model.state_dict(), os.path.join(project_dir, 'best_day2.pth'))
    print("New best model saved")

